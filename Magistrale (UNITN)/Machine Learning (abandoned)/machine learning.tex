\documentclass[a4paper, 10pt, titlepage]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[makeroom]{cancel}
\usepackage{ulem}
\usepackage{parcolumns}
\usepackage{multicol}
\setlength{\columnseprule}{0.4pt}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[T1]{fontenc}
\usepackage[margin=3cm]{geometry}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{listings}
\lstset{
	inputencoding=utf8,
	basicstyle=\ttfamily,
	tabsize=4,
	showstringspaces=false,
	literate={à}{{\`a}}1
}
\usepackage{fancyvrb}
\pagestyle{fancy}
\usepackage{epigraph}
\setlength\epigraphwidth{.8\textwidth}
\setlength\epigraphrule{0pt}
\lhead{\nouppercase{\leftmark}}
\rhead{\nouppercase{\rightmark}}

\begin{document}

	\title{Machine Learning}
	\author{Edoardo Righi}
	\maketitle
	
	\section{Introduction}
	\epigraph{A computer program is said to \textbf{learn} from experience E with respect to some class of tasks T and perfor-
mance measure P, if its performance at tasks in T, as measured by P, improves with experience}{E.T. Mitchell}

Successful applications of Machine Learning:
\begin{itemize}
\item Speech recognition, Optical character recognition, Computer Vision
\item Learning to drive an autonomous vehicle (DARPA Grand Challenges, Google Self-Driving Car)
\item Game playing (IBM’s Deep Blue, Watson, AlphaGO)
\item Recommender Systems
\end{itemize}
At the moment, big players are heavily investing in machine learning:
Google, Facebook, Amazon, IBM, Uber, ...\\
To make it effective, the components of a learning problem have to be well-posed:
\begin{itemize}
\item \textbf{task}: to be addressed by the system (e.g. recognizing handwritten characters)
\item \textbf{performance measure}: to evalute the learned system (e.g. number of misclassified characters)
\item \textbf{training experience}: to train the learning system (e.g. labelled handwritten characters)
\end{itemize}
\subsection{Designing a machine learning system}
\begin{enumerate}
\item Formalize the learning task
\item Collect data
\item Extract features
\item Choose class of learning models
\item Train model
\item Evaluate model
\end{enumerate}
\subsection{Formalize the learning task}
\begin{itemize}
\item Define the task that should be addressed by the learning system (e.g. recognizing handwritten characters from images)
\item A learning problem is often composed of a number of related tasks. E.g.:
\begin{itemize}
\item Segment the image into words and each word into characters.
\item Identify the language of the writing.
\item Classify each character into the language alphabet.
\end{itemize}
\item Choose an appropriate performance measure for evaluating the learned system (e.g. number of misclassified
characters)
\end{itemize}

\subsection{Collect data}
A set of training examples need to be collected in machine readable format. Data collection is often the most cumbersome part of the process, implying manual intervention especially in labelling examples for supervised learning.
Recent approaches to the problem of data labeling try to make use of the much cheaper availability of unlabeled
data (semi-supervised learning).

\subsection{Extract features}
A relevant set of features need to be extracted from the data in order to provide inputs to the learning system. Prior knowledge is usually necessary in order to choose the appropriate features for the task at hand. 
\begin{itemize}
\item Too few features can miss relevant information preventing the system from learning the task with reasonable
performance. 
\item Including noisy features can make the learning problem harder.
\item Too many features can require a number of examples greater than those available for training.
\end{itemize}

\subsection{Choose learning model class}
\begin{itemize}
\item A simple model like a linear classifier is easy to train but insufficient for non linearly separable data.
\begin{center}
  \makebox[.25\textwidth]{\includegraphics[width=.25\paperwidth]{dots1.png}}
\end{center}
\item A too complex model can memorize noise in training data failing to generalize to new examples.
\begin{center}
  \makebox[.3\textwidth]{\includegraphics[width=.3\paperwidth]{dots2.png}}
\end{center}
\end{itemize}

\subsection{Train model}
Training a model implies searching through the space of possible models (aka hypotheses) given the chosen model class.
Such search typically aims at fitting the available training examples well according to the chosen performance measure.
However, the learned model should perform well on unseen data (generalization), and not simply memorize training examples (overfitting).
Different techniques can be used to improve generalization, usually by trading off model complexity with training set fitting.

\epigraph{Entities are not to be multiplied without necessity}{William of Occam (Occam’s razor)}

\subsection{Evaluate model}
The learned model is evaluated according to its ability to generalize to unseen examples.
\begin{itemize}
\item Evaluation can provide insights into the model weaknesses and suggest directions for refining/modifying it.
\item Evaluation can imply comparing different models/learners in order to decide the best performing one
\end{itemize}
Statistical significance of observed differences between performance of different models should be assessed with appropriate statistical tests.

\subsection{Learning settings}
\subsubsection{Supervised learning}
\begin{itemize}
\item The learner is provided with a set of input/output pairs $(x_i , y_i) \in X \times Y$
\item The learned model f : $X \rightarrow Y$ should map input examples into their outputs (e.g. classify character images into
the character alphabet)
\item A domain expert is typically involved in labeling input examples with the corresponding outputs.
\end{itemize}
\subsubsection{Unsupervised learning}
\begin{itemize}
\item The learner is provided with a set of input examples $x_i \in X$ , with no labeling information
\item The learner models training examples, e.g. by grouping them into clusters according to their similarity
\end{itemize}
\subsubsection{Semi-supervised learning}
\begin{itemize}
\item The learner is provided with a set of input/output pairs $(x_i , y_i ) \in X \times Y$
\item $A$ (typically much bigger) additional set of unlabeled examples $x_i \in X$ is also provided.
\item Like in supervised learning, the learned model $f : X \rightarrow Y$ should map input examples into their outputs
\item Unlabelled data can be exploited to improve performance, e.g. by forcing the model to produce similar outputs for similar inputs, or by allowing to learn a better internal representation of examples.
\end{itemize}
\subsubsection{Reinforcement learning}
The learner is provided a set of possible states $S$, and for each state, a set of possible actions, $A$ moving it to a next state.
In performing action $a$ from state $s$, the learner is provided an immediate reward $r(s, a)$.
The task is to learn a policy allowing to choose for each state $s$ the action $a$ maximizing the overall reward (including future moves).
The learner has to deal with problems of \textit{delayed reward} coming from future moves, and trade-off between
\textit{exploitation} and \textit{exploration}.
Typical applications include moving policies for robots and sequential scheduling problems in general.

\subsection{Supervised learning tasks}
\subsubsection{Classification}
\begin{itemize}
\item \textbf{binary}: Assign an example to one of two possible classes (often a positive and a negative one). E.g. digit vs non-digit
character.
\item \textbf{multiclass}: Assign an example to one of $n > 2$ possible classes. E.g. assign a digit character among \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\}.
\item \textbf{multilabel}: Assign an example to a subset $m \leq n$ of the possible classes. E.g. predict the topics of a text.
\end{itemize}

\subsubsection{Regression}
Assign a real value to an example. E.g. predict biodegradation rate of a molecular compound under aerobic conditions.

\subsubsection{Ordinal regression or ranking}
Order a set of examples according to their relative importance/quality wrt the task. E.g. order emails according to
their urgency.

\subsection{Unsupervised learning tasks}
\subsubsection{Dimensionality reduction}
Reduce dimensionality of the data maintaining as much information as possible. E.g. principal component analysis (PCA), random projections.

\subsubsection{Clustering}
Cluster data into homogeneous groups according to their similarity. E.g. cluster genes according to their expression levels.

\subsubsection{Novelty detection}
Detect novel examples which differ from the distribution of a certain set of data. E.g. recognize anomalous network traffic indicating a possible attack.

\subsubsection{Probabilistic Reasoning}
\begin{itemize}
\item Reasoning in presence of uncertainty.
\item Evaluating the effect of a certain piece of evidence on other related variables.
\item Estimate probabilities and relations between variables from a set of observations.
\end{itemize}

\subsubsection{Choice of Learning Algorithms}
Information available:
\begin{itemize}
\item Full knowledge of probability distributions of data: Bayesian decision theory
\item Form of probabilities known, parameters unknown: Parameter estimation from training data
\item Form of probabilities unknown, training examples available: discriminative methods, do not model input data (generative methods), learn a function predicting the
desired output given the input
\item Form of probabilities unknown, training examples unavailable (only inputs): unsupervised methods, cluster examples by similarity
\end{itemize}

\newpage
\section{Decision Trees}
A decision tree represents a disjunction of conjunctions of constraints over attribute values.
Each path from the root to a leaf is a conjunction of the constraints specified in the nodes along it:
$$OUTLOOK = Overcast \wedge LESSON = T heoretical$$
The leaf contains the label to be assigned to instances reaching it. The disjunction of all paths is the logical formula represented by the tree.

\subsection{Appropriate problems}
\begin{itemize}
\item Binary or multiclass classification tasks (extensions to regressions also exist)
\item Instances represented as attribute-value pairs
\item Different explanations for the concept are possible (disjunction)
\item Some instances have missing attributes
\item There is need for an interpretable explanation of the output
\end{itemize}

\subsection{Learning decision trees}
\begin{itemize}
\item Greedy top-down strategy (ID3 - Quinlan 1986, C4-5 - Quinlan 1993)
\item For each node, starting from the root with full training set:
\begin{enumerate}
\item Choose best attribute to be evaluated
\item Add a child for each attribute value
\item Split node training set into children according to value of chosen attribute
\item Stop splitting a node if it contains examples from a single class, or there are no more attributes to test.
\end{enumerate}
\item Divide et impera approach
\end{itemize}

\subsection{Chosing the best attribute}
\subsubsection{Entropy}
Entropy is a measure of the amount of information contained in a collection of instances S which can take a number c of possible values:
$$H(S) = - \sum_{i=1}^{c}{p_i log_2p_i}$$
where $p_i$ is the fraction of $S$ taking value $i$. \\
In our case instances are training examples and values are class labels. The entropy of a set of labelled examples measures its label inhomogeneity.

\subsubsection{Information gain}
Expected reduction in entropy obtained by partitioning a set $S$ according to the value of a certain attribute $A$:
$$IG(S,A) = H(S) - \sum_{v \in Values(A)} \dfrac{|S_v|}{|S|}H(S_v)$$
where $Values(A)$ is the set of possible values taken by $A$ and $S_v$ is the subset of $S$ taking value $v$ at attribute $A$. \\
The second term represents the sum of entropies of subsets of examples obtained partitioning over A values, weighted by their respective sizes. An attribute with high information gain tends to produce homogeneous groups in terms of labels, thus favouring their classification.

\subsection{Issues in decision tree learning}
\subsubsection{Overfitting avoidance}
Requiring that each leaf has only examples of a certain class can lead to very complex trees. A complex tree can easily overfit the training set, incorporating random regularities not representative of the full distribution, or noise in the data. It is possible to accept impure leaves, assigning them the label of the majority of their training examples. \\
Two possible strategies to prune a decision tree:
\begin{itemize}
\item \textbf{pre-pruning} decide whether to stop splitting a node even if it contains training examples with different labels.
\item \textbf{post-pruning} learn a full tree and successively prune it removing subtrees.
\end{itemize}

\subsubsection{Reduced error pruning}
It's a post-pruning strategy. It assumes a separate labelled validation set for the pruning stage. Procedure:
\begin{enumerate}
\item For each node in the tree, evaluate the performance on the validation set when removing the subtree rooted at it
\item If all node removals worsen performance, STOP.
\item Choose the node whose removal has the best performance improvement
\item Replace the subtree rooted at it with a leaf
\item Assign to the leaf the majority label of all examples in the subtree
\item Return to 1
\end{enumerate}

\subsubsection{Dealing with continuous-valued attributes}
Continuous valued attributes need to be discretized in order to be used in internal nodes tests. Discretization threshold can be chosen in order to maximize the attribute quality criterion (e.g. infogain). \\
Procedure:
\begin{enumerate}
\item Examples are sorted according to their continuous attribute values
\item For each pair of successive examples having different labels, a candidate threshold is placed as the average of the two attribute values.
\item For each candidate threshold, the infogain achieved splitting examples according to it is computed
\item The threshold producing the higher infogain is used to discretize the attribute
\end{enumerate}

\subsubsection{Alternative attribute test measures}
The information gain criterion tends to prefer attributes with a large number of possible values. As an extreme, the unique ID of each example is an attribute perfectly splitting the data into singletons, but it will be of no use on new examples.
A measure of such spread is the entropy of the dataset wrt the attribute value instead of the class value:
$$H_A(S) = - \sum_{v \in Values(a)}{\dfrac{|S_v|}{|S|} log_2 \dfrac{|S_v|}{|S|}}$$
The gain ratio measure downweights the information gain by such attribute value entropy:
$$IGR(S,A) = \dfrac{IG(S,A}{H_A(S)}$$

\subsubsection{Handling attributes with missing values}
Assume example x with class c(x) has missing value for attribute A.
When attribute A is to be tested at node n, the complex solution implies that at test time, for each candidate class, all fractions of the test example which reached a leaf with that class are summed, and the example is assigned the class with highest overall value.

\newpage

\section{K-Nearest Neighbour}
\subsection{1-Nearest Neighbour classification}
Metric or distance definition:
Given a set $X$, a function $d : X \times X \rightarrow \mathbb{R}_{0}^{+} + 0$ is a metric for $X$ if for any $x, y, z \in X$ the following properties are
satisfied:
\begin{itemize}
\item reflexivity $d(x, y) = 0 \quad \text{iff} \quad \text{x = y}$
\item symmetry $d(x, y) = d(y, x)$
\item triangle inequality $d(x, y) + d(y, z) \geq d(x, z)$
\end{itemize}

\subsection{k-Nearest Neighbour classification}
\begin{lstlisting}
	for all test examples x do
		for all training examples $(x_i , y_i )$ do
			compute distance $d(x, x_i )$	
		end for
		select the k-nearest neighbours of x
		return class of x as majority class among neighbours:
\end{lstlisting}
$$argmax_y \sum_{i=1}^{k}\delta(y,y_i)$$
\begin{lstlisting}
	end for
\end{lstlisting}
Note:
\begin{equation}
delta(x,y) =  \begin{cases}
      1 & \text{if \quad x=y}\\
      0 & \text{otherwise}
    \end{cases}   
\end{equation}

\subsection{k-Nearest Neighbour regression}
\begin{lstlisting}
	for all test examples x do
		for all training examples (x i , y i ) do
			compute distance d(x, x i )
		end for
		select the k-nearest neighbours of x
		return the average output value among neighbours:
\end{lstlisting}
$$\dfrac{1}{k} \sum_{i=1}^{k}y_i$$
\begin{lstlisting}
	end for
\end{lstlisting}

\subsection{Characteristics of k-nearest neighbour learning}
\begin{itemize}
\item \textbf{instance-based learning} the model used for prediction is calibrated for the test example to be processed
\item \textbf{lazy learning} computation is mostly deferred to the classification phase
\item \textbf{local learner} assumes prediction should be mainly influenced by nearby instances
\item \textbf{uniform feature weighting} all features are uniformly weighted in computing distances
\end{itemize}

\subsection{Distance-weighted k-nearest neighbour}
Classification:
$$argmax_y \sum_{i=1}^{k} w_i \delta(y,y_i)$$
Regression:
$$\dfrac{\sum_{i=1}^{k}w_i y_i}{\sum_{i=1}^{k} w_i}$$
where: $w_i = \dfrac{1}{d(x,x_i)}$
\newpage

\section{Linear algebra}
\subsection{Vector space}
A set $X$ is called a vector space over $\mathbb{R}$ if addition and scalar multiplication are defined and satisfy for all $x, y, z \in
$ and $\mu$, $\sigma \in \mathbb{R}$:
\begin{itemize}
\item Addition:	
	\begin{itemize}
	\item \textbf{associative} $x + (y + z) = (x + y) + z$
	\item \textbf{commutative} $x + y = y + x$
	\item \textbf{identity element} $\exists 0 \in X : x + 0 = x$
	\item \textbf{inverse element} $\forall x \in X \; \exists x' \in X : x + x' = 0$
	\end{itemize}
\item Scalar multiplication:
	\begin{itemize}
	\item \textbf{distributive over elements} $\lambda(x + y) = \lambda x + \lambda y$
	\item \textbf{distributive over scalars} $(\lambda + \mu)x = \lambda x + \mu x$
	\item \textbf{associative over scalars} $\lambda (\mu x) = (\lambda\mu)x$
	\item \textbf{identity element} $\exists 1 \in \mathbb{R} : 1x = x$
	\end{itemize}
\end{itemize}
\subsubsection{Properties and operations in vector spaces}
\begin{itemize}
\item \textbf{subspace}: any non-empty subset of X being itself a vector space (E.g. projection)
\item \textbf{linear combination}: given $\lambda_i \in \mathbb{R}$, $x_i \in X$
$$\sum_{i=1}^{n} \lambda_i x_i$$
\item 	\textbf{span}: the span of vectors $x_1, \dots , x_n$ is defined as the set of their linear combinations
$$\left\lbrace \sum_{i=1}^{n} \lambda_i x_i, \quad \lambda_i \in \mathbb{R} \right\rbrace$$
\end{itemize}

\subsubsection{Basis in vector spaces}
\begin{itemize}
\item \textbf{Linear independency} A set of vectors x i is linearly independent if none of them can be written as a linear combination of the others.
\item \textbf{Basis} A set of vectors $x_i$ is a basis for $X$ if any element in $X$ can be uniquely written as a linear combination of vectors $x_i$. Necessary condition is that vectors $x_i$ are linearly independent. All bases of $X$ have the same number of elements, called the \textit{dimension} of the vector space.
\end{itemize}


\newpage
\section{Probability theory}
\section{Evaluation}
\section{Bayesian decision theory}


\end{document} 
